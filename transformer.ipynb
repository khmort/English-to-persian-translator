{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def break_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Breaking sentence into its constituent words\n",
    "\n",
    "    Args:\n",
    "        sentence (str): A sentence you want to extract words from\n",
    "    \n",
    "    Returns:\n",
    "        list: List of words that make up the sentence\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    # Removing the <sos> & <eos> from sentence\n",
    "    # This is because when tokenizing this sentence by Tokenizer,\n",
    "    # the SOS and EOS signals will be added to the beginning and end of it!\n",
    "    sentence = re.sub(\"<\\s*[se]\\s*o\\s*s\\s*>\", \"\", sentence)\n",
    "    words = re.split(f\"[\\n\\s0-9{string.punctuation}]+\", sentence)\n",
    "    words = words if words[0] else words[1:]\n",
    "    words = words if words[-1] else words[:-1]\n",
    "    return words\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sos = \"<sos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.words = [self.sos, self.eos]\n",
    "\n",
    "    def extract_words(self, sentence: str):\n",
    "        \"\"\"\n",
    "        Extracting and adding words to the dictionary\n",
    "        \"\"\"\n",
    "        # Adding <Start-of-sequence> and <End-of-sequence> words to the beginning and end of the sentence\n",
    "        words = [self.sos] + break_sentence(sentence) + [self.eos]\n",
    "\n",
    "        # Adding all words to the dictionary\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if not word in self.words:\n",
    "                self.words.append(word)\n",
    "    \n",
    "    def extract_all_words(self, sentences: list):\n",
    "        \"\"\"\n",
    "        Extract all words in a list of sentences\n",
    "        \"\"\"\n",
    "        for sentence in sentences:\n",
    "            self.extract_words(sentence)\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        \"\"\"\n",
    "        Converting a sentence into its constituent tokens\n",
    "\n",
    "        Args:\n",
    "            sentence (str): The sentence you want to tokenize\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of tokens representing the words in the sentence\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = []\n",
    "        # Adding <SOS> & <EOS> to the sentence: <SOS> sentence ... <EOS>\n",
    "        words = [self.sos] + break_sentence(sentence) + [self.eos]\n",
    "        for w in words:\n",
    "            w = w.lower()\n",
    "            tokens.append(self.words.index(w) + 1 if w in self.words else 0)\n",
    "        return tokens\n",
    "    \n",
    "    def tokenize_all(self, sentences):\n",
    "        \"\"\"\n",
    "        Input a list of sentences and get a list of token lists\n",
    "        \"\"\"\n",
    "        return [self.tokenize(sen) for sen in sentences]\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.words) + 1\n",
    "    \n",
    "    def get_max_length(self, sentences: list):\n",
    "        \"\"\"\n",
    "        Give a list of sentences and this function will calculate the maximum length of a sentence in this list.\n",
    "        If list is empty it will return -1\n",
    "        \"\"\"\n",
    "        mx = -1\n",
    "        for sen in sentences:\n",
    "            mx = max(mx, len(self.tokenize(sen)))\n",
    "        return mx\n",
    "    \n",
    "    def to_string(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert a list of tokens into a corresponding sentence.\n",
    "        \"\"\"\n",
    "        text = []\n",
    "        for token in tokens:\n",
    "            text.append(\"<unk>\" if token == 0 else self.words[token - 1])\n",
    "        return ' '.join(text)\n",
    "    \n",
    "    def get_index(self, word: str):\n",
    "        \"\"\"\n",
    "        Find the index or token of a word in the dictionary of words.\n",
    "        If there is no such word in dictionary, it will return 0=<UNKNOWN>\n",
    "        \"\"\"\n",
    "        return self.words.index(word) + 1 if word in self.words else 0\n",
    "\n",
    "def pad_sequence(tokens_seq):\n",
    "    \"\"\"\n",
    "    Adding zero tokens to the end of sentences to match the length of the longest sentence in the list.\n",
    "    For example:\n",
    "        [[1, 5, 7, 2, 0]\n",
    "         [1, 3, 2, 0, 0]\n",
    "         [1, 2, 0, 0, 0]\n",
    "         [1, 3, 4, 5, 2]]\n",
    "    \"\"\"\n",
    "    new_seq = []\n",
    "    max_size = max(len(seq) for seq in tokens_seq)\n",
    "    for tokens in tokens_seq:\n",
    "        new_seq.append(tokens + [0] * (max_size - len(tokens)))\n",
    "    return new_seq\n",
    "\n",
    "\n",
    "# Test tokenizer\n",
    "tknizer = Tokenizer()\n",
    "sentences = [\n",
    "    \"Hello, World!\",\n",
    "    \"Microsoft was founded by Bill Gates.\",\n",
    "    \"Google is the most powerful search engine.\",\n",
    "    \"Ronnie Coleman is an eight-time Mr. Olympia champion!\",\n",
    "    \"Hello, how are you?\"\n",
    "]\n",
    "tknizer.extract_all_words(sentences)\n",
    "print(tknizer.words)\n",
    "\n",
    "sentence = \"Who was Google founded by?\"\n",
    "tokens = tknizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "print(tknizer.to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb9fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_input, d_output, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_output = d_output\n",
    "        self.d_input = d_input\n",
    "        self.token_emdedding = torch.nn.Embedding(vocab_size, d_output, padding_idx=0)\n",
    "        self.pos_enc = self.positional_encoding()\n",
    "\n",
    "    def positional_encoding(self):\n",
    "        pe = torch.zeros(self.d_input, self.d_output)\n",
    "        position = torch.arange(0, self.d_input).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_output, 2) * (-math.log(10000.0) / self.d_output))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emdedding(x)\n",
    "        x += self.pos_enc\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376bcc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.q_linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.k_linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.v_linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        attention_weight = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.output_dim)\n",
    "        if not mask is None:\n",
    "            attention_weight = attention_weight.masked_fill(~mask, 0.0)\n",
    "        \n",
    "        return torch.matmul(attention_weight, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6146b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_head):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_head = max(n_head, 1)\n",
    "        self.heads = []\n",
    "        for _ in range(self.n_head):\n",
    "            self.heads.append(Attention(input_dim, output_dim))\n",
    "        self.output_linear = torch.nn.Linear(n_head * output_dim, output_dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        X = self.heads[0](q, k, v, mask)\n",
    "        for head in self.heads[1:]:\n",
    "            X = torch.cat([X, head(q, k, v, mask)], dim=2)\n",
    "        return self.output_linear(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10b0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, output_dim, vocab_size, n_head):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEmbedding(input_dim, embedding_dim, vocab_size)\n",
    "        self.mha = MultiHeadAttention(embedding_dim, output_dim, n_head)\n",
    "        self.normlinear1 = torch.nn.Linear(embedding_dim, output_dim)\n",
    "        self.normlinear2 = torch.nn.Linear(output_dim, output_dim)\n",
    "        self.layernorm = torch.nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pe(x)\n",
    "        y = self.mha(x, x, x)\n",
    "        y = y + self.normlinear1(x)\n",
    "        return self.layernorm(self.normlinear2(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f739bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, output_dim, vocab_size, n_head):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEmbedding(input_dim, embedding_dim, vocab_size)\n",
    "        self.masked_mha = MultiHeadAttention(embedding_dim, output_dim, n_head)\n",
    "        self.combine_mha = MultiHeadAttention(output_dim, output_dim, n_head)\n",
    "\n",
    "        self.norm1 = torch.nn.Linear(embedding_dim, output_dim)\n",
    "        self.norm2 = torch.nn.Linear(output_dim, output_dim)\n",
    "        self.norm3 = torch.nn.Linear(output_dim, output_dim)\n",
    "\n",
    "        self.layernorm = torch.nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, tgt_tokens, encoder_output, src_padding_mask=None):\n",
    "        x = self.pe(tgt_tokens)\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        tgt_mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).bool()\n",
    "        tgt_mask = tgt_mask.unsqueeze(0).expand(x.size(0), -1, -1)  # [batch, tgt_len, tgt_len]\n",
    "\n",
    "        # Decoder self-attention\n",
    "        y = self.masked_mha(x, x, x, mask=tgt_mask)\n",
    "        x = self.layernorm(y + self.norm1(x))\n",
    "\n",
    "        x2 = self.combine_mha(x, encoder_output, encoder_output)\n",
    "        x = x2 + self.norm2(x)\n",
    "\n",
    "        return self.norm3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e5f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(torch.nn.Module):\n",
    "    def __init__(self, enc_input_dim, dec_input_dim, embedding_dim, output_dim, enc_vocab_size, dec_vocab_size, n_head):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(enc_input_dim, embedding_dim, output_dim, enc_vocab_size, n_head)\n",
    "        self.decoder = Decoder(dec_input_dim, embedding_dim, output_dim, dec_vocab_size, n_head)\n",
    "        self.linear = torch.nn.Linear(output_dim, dec_vocab_size)\n",
    "        self.enc_input_dim = enc_input_dim\n",
    "        self.dec_input_dim = dec_input_dim\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        encode = self.encoder(x)\n",
    "        decode = self.decoder(y, encode)\n",
    "        return self.linear(decode)\n",
    "    \n",
    "    def translate(self, org_sentence, org_tokenizer: Tokenizer, dest_tokenizer: Tokenizer):\n",
    "\n",
    "        dest_start_token = dest_tokenizer.get_index('<sos>')\n",
    "        dest_end_token = dest_tokenizer.get_index('<eos>')\n",
    "\n",
    "        input_tokens = org_tokenizer.tokenize(org_sentence)\n",
    "        input_tokens = input_tokens + [0] * max(self.enc_input_dim - len(input_tokens), 0)\n",
    "        input_tokens = torch.tensor(input_tokens).unsqueeze(0)\n",
    "\n",
    "        output = torch.tensor([[dest_start_token] + [0] * (self.dec_input_dim - 1)])\n",
    "        target_index = 0\n",
    "        result = [dest_start_token]\n",
    "        \n",
    "        next_word = dest_start_token\n",
    "        while next_word != dest_end_token and target_index < self.dec_input_dim - 1:\n",
    "            logits = self.forward(input_tokens, output)\n",
    "            indices = torch.argmax(logits, dim=2)\n",
    "            next_word = indices[0, target_index]\n",
    "            result.append(next_word.item())\n",
    "            output[0, target_index + 1] = next_word\n",
    "            target_index += 1\n",
    "        \n",
    "        return dest_tokenizer.to_string(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529c20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baac7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = read_lines(\"test.en\")\n",
    "farsi_sentences = read_lines(\"test.fa\")\n",
    "\n",
    "eng_tokenizer = Tokenizer()\n",
    "fas_tokenizer = Tokenizer()\n",
    "eng_tokenizer.extract_all_words(english_sentences)\n",
    "fas_tokenizer.extract_all_words(farsi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a2157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    enc_input_dim=eng_tokenizer.get_max_length(english_sentences),\n",
    "    dec_input_dim=fas_tokenizer.get_max_length(farsi_sentences),\n",
    "    embedding_dim=32,\n",
    "    output_dim=64,\n",
    "    enc_vocab_size=eng_tokenizer.vocab_size(),\n",
    "    dec_vocab_size=fas_tokenizer.vocab_size(),\n",
    "    n_head=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf9366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(pad_sequence(eng_tokenizer.tokenize_all(english_sentences)))\n",
    "output = torch.tensor(pad_sequence(fas_tokenizer.tokenize_all(farsi_sentences)))\n",
    "real = torch.tensor(pad_sequence(\n",
    "    [sen[1:] + [0] for sen in fas_tokenizer.tokenize_all(farsi_sentences)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a135fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "cost = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(translator.parameters(), lr=0.01)\n",
    "\n",
    "dataset = TensorDataset(input, output, real)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    total_loss = 0.0\n",
    "    batch_count = 1\n",
    "\n",
    "    for org_input, des_output, des_real in loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = translator(org_input, des_output)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        target = des_real.view(-1)\n",
    "        loss = cost(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {(total_loss / len(loader)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b98ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.translate(\"I am relatively free about the date\", eng_tokenizer, fas_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
